{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangsoyoung10011001/machinelearning/blob/main/searchEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mHTxbdY_L6G",
        "outputId": "687803a0-192f-4b9b-d636-f9871e070ff8"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44-yGLhNw-b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "e0f1b924-e6d4-4551-fa5f-7d6119b5cfb9"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# -------------------cran.all.1400-------------------\n",
        "# read cran.all.1400 file\n",
        "f=open('/content/drive/Shareddrives/머신러닝팀플/cran/cran.all.1400.txt', 'r')\n",
        "cranAll=f.read()\n",
        "f.close()\n",
        "\n",
        "# split by .I\n",
        "cranAll=cranAll.split('.I')\n",
        "cranAll.pop(0)\n",
        "\n",
        "# store each .W in list\n",
        "cranAllList=[]\n",
        "for i in range(len(cranAll)):\n",
        "    cranAllList.append(cranAll[i].split('.W')[1])\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# -------------------cran.qry-------------------\n",
        "# read cran.qry file\n",
        "f=open('/content/drive/Shareddrives/머신러닝팀플/cran/cran.qry.txt', 'r')\n",
        "cranQry=f.read()\n",
        "f.close()\n",
        "\n",
        "# split by .I\n",
        "cranQry=cranQry.split('.I')\n",
        "cranQry.pop(0)\n",
        "\n",
        "\n",
        "# store each .W in list\n",
        "cranQryList=[]\n",
        "for i in range(len(cranQry)):\n",
        "    cranQryList.append(cranQry[i].split('.W')[1])\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "#--------------------------cranqrel-----------------------------------\n",
        "f=open('/content/drive/Shareddrives/머신러닝팀플/cran/cranqrel.txt', 'r')\n",
        "cranqrel=f.read()\n",
        "f.close()\n",
        "\n",
        "# split by -1\n",
        "cranqrel=cranqrel.split('-1')\n",
        "\n",
        "cranqrelList=[]\n",
        "for i in range(len(cranqrel)):\n",
        "    cranqrelList.append(cranqrel[i].split('\\n'))\n",
        "for i in range(1, len(cranqrel)):\n",
        "  cranqrelList[i].pop(0)\n",
        "\n",
        "cranqrelListList=[]\n",
        "for i in range(len(cranqrel)-1):\n",
        "  tempList=[]\n",
        "  for j in range(len(cranqrelList[i])):\n",
        "    tempList.append(cranqrelList[i][j].split(' ')[1])\n",
        "  tempList.pop(len(tempList)-1)\n",
        "  cranqrelListList.append(tempList)\n",
        "#--------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-377e06699d2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# -------------------cran.all.1400-------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# read cran.all.1400 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/Shareddrives/머신러닝팀플/cran/cran.all.1400.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcranAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Shareddrives/머신러닝팀플/cran/cran.all.1400.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6p4GIRz-EUF"
      },
      "source": [
        "# list sorting function\n",
        "def wordSorting(wordList):\n",
        "  wordList.sort()\n",
        "\n",
        "  # return sorted list\n",
        "  return wordList\n",
        "\n",
        "# function make sorted list of words in query\n",
        "def makeQueryWordList(query):\n",
        "    cVec = CountVectorizer(stop_words='english')\n",
        "    tf = cVec.fit_transform([query])\n",
        "    queryWordList = list(cVec.vocabulary_)\n",
        "\n",
        "    # return sorted list of words in query\n",
        "    return wordSorting(queryWordList)\n",
        "\n",
        "def makeDocIndexList(tf, totalWordDict, queryWordList, indexOfQuery, docLen):\n",
        "  docIndexListList=[]\n",
        "  # query에 있는 단어 개수만큼 for\n",
        "  for i in queryWordList[indexOfQuery]:\n",
        "    wordIndex=totalWordDict.get(i)\n",
        "\n",
        "    docIndexList=[]\n",
        "        # doc 개수만큼 for\n",
        "    for j in range(docLen):\n",
        "            # if(tf[j, wordIndex]==None):\n",
        "            #     continue\n",
        "      if(wordIndex!=None):\n",
        "        if(tf[j, wordIndex]!=0):\n",
        "          docIndexList.append(j)\n",
        "    docIndexListList.append(docIndexList)\n",
        "\n",
        "    # return list of document index for each word in query\n",
        "  return docIndexListList\n",
        "\n",
        "# list to store sorted words in each query\n",
        "q_wordListList=[]\n",
        "for j in range(len(cranQryList)):\n",
        "  q_wordListList.append(makeQueryWordList(cranQryList[j]))\n",
        "\n",
        "# make list for CountVectorizer\n",
        "cranAllListList=[]\n",
        "for i in range(len(cranAllList)):\n",
        "    cranAllListList.append(cranAllList[i])\n",
        "# compute tf\n",
        "cVec=CountVectorizer(stop_words='english')\n",
        "tf=cVec.fit_transform(cranAllListList)\n",
        "# make list of total words in documents\n",
        "totalWordList=list(cVec.vocabulary_.keys())\n",
        "totalWordList=wordSorting(totalWordList)\n",
        "\n",
        "#input('Enter query index: ')\n",
        "queryIndex=int(input('Enter query index: '))-1\n",
        "docIndexListList=makeDocIndexList(tf, cVec.vocabulary_, q_wordListList, queryIndex, len(cranAllListList))\n",
        "\n",
        "# make inverted index table\n",
        "df=pd.DataFrame(q_wordListList[queryIndex], columns=['Term'])\n",
        "df['Document']=docIndexListList\n",
        "# print(df)\n",
        "\n",
        "# list to store document index without duplication\n",
        "totalDocIndexList=[]\n",
        "for i in range(len(df)):\n",
        "    for j in range(len(df[i:i+1]['Document'].values[0])):\n",
        "        totalDocIndexList.append(df[i:i+1]['Document'].values[0][j])\n",
        "\n",
        "\n",
        "#____________________________remove duplication ______________________________________________\n",
        "\n",
        "a=list(totalDocIndexList)\n",
        "a1=list(totalDocIndexList)\n",
        "\n",
        "a2=[]\n",
        "\n",
        "for e in range(0,len(a1)):\n",
        "  k=a1[e]\n",
        "  if k in a:\n",
        "    a2.append(k)\n",
        "    while k in a:\n",
        "      a.remove(k)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_G96DRgid7u"
      },
      "source": [
        "\n",
        "###---------------------------------------------\n",
        "\n",
        "totalDocIndexList=a2\n",
        "\n",
        "numDoc=len(totalDocIndexList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhd7vsSFQHqC"
      },
      "source": [
        "89번 sorted(a2) 고치면 값바뀜\n",
        "query 0번째에서 sorted하면 precision, recall 0나옴\n",
        "sorted(a2)하지말기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9R9YwYnLDlD"
      },
      "source": [
        "#-----------------------------------------------------------------\n",
        "# list to store query and document\n",
        "listToCVec=[]\n",
        "# listToCVec.append(cranQryList[queryIndex])\n",
        "for i in range(numDoc):\n",
        "    listToCVec.append(cranAllList[totalDocIndexList[i]])\n",
        "cVec = CountVectorizer(stop_words='english', max_df=0.5)\n",
        "sf = cVec.fit_transform(listToCVec)\n",
        "transformer = TfidfTransformer(smooth_idf =False)\n",
        "transformed_weights= transformer.fit_transform (sf)\n",
        "\n",
        "query_sf = cVec.transform([cranQryList[queryIndex]])\n",
        "query_weight = transformer.transform(query_sf)\n",
        "\n",
        "docW = transformed_weights.toarray()\n",
        "queryW =query_weight.toarray()\n",
        "\n",
        "sim=docW*queryW\n",
        "\n",
        "sim_sum = pd.DataFrame(columns=['Doc','Similarity'])\n",
        "\n",
        "for j in range(0,numDoc):\n",
        "  sim_sum=sim_sum.append({'Doc':j+1,'Similarity':sim[j].sum()},ignore_index=True)\n",
        "\n",
        "order=sim_sum.sort_values('Similarity',ascending=False)\n",
        "order_list=list(order['Doc'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_-fDV5lMkvD"
      },
      "source": [
        "#test\n",
        "order_list_str=[]\n",
        "for k in order_list:\n",
        "  order_list_str.append(str(int(k)))\n",
        "\n",
        "sameList2=[]\n",
        "for i in order_list_str:\n",
        "  if i in cranqrelListList[0]:\n",
        "    sameList2.append(i)        \n",
        "  else:\n",
        "    continue \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXvi0TiWhfsQ"
      },
      "source": [
        "여기서 k list받음\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJNqLyZ7-ipW"
      },
      "source": [
        "# order=sim_sum.sort_values('Similarity',ascending=False)\n",
        "# order_list=list(order['Doc'])\n",
        "# print(order_list)\n",
        "\n",
        "#----------sort_doc_Ranked by K-----------\n",
        "def sort_docRank(k_list,order_list2):\n",
        "  c=[]\n",
        "  for i in k_list:\n",
        "    b=order_list2[0:i]\n",
        "    c.append(b)\n",
        "  return c\n",
        "k=[3,5]\n",
        "rank_doc_query=sort_docRank(k,order_list_str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQpFU-cLDFKm"
      },
      "source": [
        "#------------find query num data----------\n",
        "def extractList_relByquery(queryIndex,k_list):\n",
        "  rel_list_want=[]\n",
        "  rel_list_want.append(cranqrelListList[int(queryIndex)])\n",
        "  return rel_list_want*len(k_list)\n",
        "  \n",
        "rel_want_byQuery=extractList_relByquery(queryIndex,k)\n",
        "print(rel_want_byQuery)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pamLB9lEkwH"
      },
      "source": [
        "#------------find false data---------------\n",
        "#print(doc_list)\n",
        "## type inverter------\n",
        "def typeinverter(list_int):\n",
        "  inverter=[]\n",
        "  for y in list_int:\n",
        "    inverter.append(str(y))\n",
        "  return(inverter)\n",
        "\n",
        "\n",
        "def falsedata(checkdata):\n",
        "  doc_list=[]\n",
        "  for j in range(0,len(checkdata)):\n",
        "    doc_list.append(typeinverter(list(np.arange(1,1401))))\n",
        "    for a in checkdata[j]:#check data = cranqrelListList or predictedListList\n",
        "      doc_list[j].remove(a) \n",
        "  # print(doc_list)\n",
        "  return(doc_list)\n",
        "\n",
        "# print(r_f)\n",
        "# print(p_f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qku_DwyQH66i"
      },
      "source": [
        "# #query=2, TT,TF,FT,FF\n",
        "# def samedata(rel,pred):#rel 데이터 1400개중에 사용할 쿼리번호 ???\n",
        "#     same_data=[]\n",
        "#     for i in range(0,len(pred)):\n",
        "#       same_data.append(list(set(rel[i].intersection(pred[i]))))\n",
        "#     print(same_data)\n",
        "\n",
        "def sequence(seq1,seq2,k_list):\n",
        "  sameList=[]\n",
        "  # diffList_list1=[]\n",
        "  # diffList_list2=[]\n",
        "#seq1과 seq2의 개수는 같을거야 근데 이중리스트일거야\n",
        "# 근데 바깥[]은 같고==원하는 쿼리개수(len(query_list)), \n",
        "#[[]]안은 다를수도있어(쿼리에 해당하는 k개의 doc번호 거나,rel파일의 관련있는 doc번호야)\n",
        "  for t in range(0,len(k_list)):\n",
        "    sameList2=[]\n",
        "    # diffList_list1=[]\n",
        "    # diffList_list2=[]\n",
        "    for i in seq1[t]:\n",
        "      # sameList2=[]\n",
        "      if i in seq2[t]:\n",
        "        sameList2.append(i)        \n",
        "      else:\n",
        "        continue \n",
        "    sameList.append(sameList2)\n",
        "  # for j in sameList:\n",
        "  #  print(j)\n",
        "    # diffList_list1=(list(filter(lambda x:x not in sameList,seq1)))\n",
        "    # diffList_list2=(list(filter(lambda x:x not in sameList,seq2)))\n",
        "  \n",
        "  # print(k)\n",
        "  return sameList#,diffList_list1,diffList_list2\n",
        "\n",
        "# print(sequence(r_f,p_f))\n",
        "# print(sequence(r_f,p_f))\n",
        "# p(rint(sequence(r_f,p_f)[2])\n",
        "rf=falsedata(rel_want_byQuery)\n",
        "# print(rf)\n",
        "pf=falsedata(rank_doc_query)\n",
        "\n",
        "rt=rel_want_byQuery\n",
        "pt=rank_doc_query\n",
        "\n",
        "\n",
        "rtpt=sequence(rt,pt,k)\n",
        "rtpf=sequence(rt,pf,k)\n",
        "rfpt=sequence(rf,pt,k)\n",
        "rfpf=sequence(rf,pf,k)\n",
        "# print(\"tp : \",len(rtpt[0]))\n",
        "# print(\"fn : \",len(rtpf[0]))\n",
        "# print(\"fp : \",len(rfpt[0]))\n",
        "# print(\"tn : \",len(rfpf[0]))\n",
        "\n",
        "\n",
        "def precision_recall (tp,fn,fp,tn,k_list):\n",
        "  precision=[]\n",
        "  recall=[]\n",
        "\n",
        "  for i in range(0,len(k_list)):\n",
        "    zerodivision=len(tp[i])+len(fp[i])\n",
        "    if zerodivision > 0:\n",
        "      recall.append(len(tp[i])/(len(tp[i])+len(fn[i])))\n",
        "      precision.append(len(tp[i])/(len(tp[i])+len(fp[i])))\n",
        "  else:\n",
        "    print(\"didn't have true positive data and false positive data\")\n",
        "  return precision,recall\n",
        "\n",
        "\n",
        "prec=precision_recall(rtpt,rtpf,rfpt,rfpf,k)[0]\n",
        "rec=precision_recall(rtpt,rtpf,rfpt,rfpf,k)[1]\n",
        "print('precision:',prec)\n",
        "print('recall:', rec)\n",
        "\n",
        "list_df = pd.DataFrame(list(zip(k, prec,rec)),columns=['k', 'precision','recall'])\n",
        "print(list_df)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z0Bl8fu2bQZ"
      },
      "source": [
        "print(list_df.sort_values(by='precision', ascending=False))\n",
        "print(list_df.sort_values(by='recall', ascending=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KgKDPflvzRe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}