# -*- coding: utf-8 -*-
"""최최종

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12pYJT_tuk9atG-cp3MA5cQxQ8bDQnIR9
"""

from matplotlib import pyplot as plt
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.mixture import GaussianMixture
from sklearn.cluster import DBSCAN

from sklearn.cluster import estimate_bandwidth
from sklearn.cluster import MeanShift
import math
import time
import warnings ; warnings.filterwarnings('ignore')
import seaborn as sns
from sklearn.metrics import accuracy_score, silhouette_samples, silhouette_score
from sklearn import preprocessing
from sklearn.model_selection import cross_val_score, StratifiedKFold 
skf = StratifiedKFold(n_splits=10) 
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
import sys
from sklearn.cluster import KMeans
from sklearn import metrics

#for scale and encorder
class PreprocessPipeline(): 
    def __init__(self, num_process, cat_process, verbose=False): 
        #super(PreprocessPipeline, self).__init__() 
        self.num_process = num_process 
        self.cat_process = cat_process 
        #for each type
        if num_process == 'standard': 
            self.scaler = preprocessing.StandardScaler() 
        elif num_process == 'minmax': 
            self.scaler = preprocessing.MinMaxScaler() 
        elif num_process == 'maxabs': 
            self.scaler = preprocessing.MaxAbsScaler() 
        elif num_process == 'robust': 
            self.scaler = preprocessing.RobustScaler() 
        else: 
            raise ValueError("Supported 'num_process' : 'standard','minmax','maxabs','robust'")   
        if cat_process == 'onehot': 
            self.encoder = preprocessing.OneHotEncoder(sparse=False, handle_unknown='ignore')  
        elif cat_process == 'ordinal': 
            self.encoder = preprocessing.OrdinalEncoder() 
        elif cat_process == 'label':
            self.encoder = preprocessing.LabelEncoder()
           
        else: 
            raise ValueError("Supported 'cat_process' : 'onehot', ordinal','label'") 
        self.verbose=verbose 
        
        #do Preprocess
    def process(self, X): 
        X_cats = X.select_dtypes(np.object).copy() 
        X_nums = X.select_dtypes(exclude=np.object).copy() 
        #Xt_cats = Xt.select_dtypes(np.object).copy() 
        #Xt_nums = Xt.select_dtypes(exclude=np.object).copy() 

        if self.verbose: 
            print(f"Categorica Colums : {list(X_cats)}") 
            print(f"Numeric Columns : {list(X_nums)}") 

        if self.verbose: 
            print(f"Categorical cols process method : {self.cat_process.upper()}") 
  
        X_cats = self.encoder.fit_transform(X_cats) 
        #Xt_cats = self.encoder.transform(Xt_cats) 
    

        if self.verbose: 
            print(f"Numeric columns process method : {self.num_process.upper()}") 
        X_nums = self.scaler.fit_transform(X_nums) 
        #Xt_nums = self.scaler.transform(Xt_nums) 
        X_nums=pd.DataFrame(X_nums)
        X_cats=pd.DataFrame(X_cats)
        X_processed = pd.concat([X_nums, X_cats],axis=1) 
        #Xt_processed = np.concatenate([Xt_nums, Xt_cats], axis=-1) 


        return X_processed

def purity_score(y_true, y_pred):
# compute contingency matrix (also called confusion matrix)
  contingency_matrix = metrics.cluster.contingency_matrix(y_true,y_pred)
  if np.sum(contingency_matrix)==0:
    print("contigency_matrix is 0")
  else:
    return np.sum(np.amax(contingency_matrix, axis=0))/np.sum(contingency_matrix)

def makeOrderList(dataframe):
  dataframe.sort_values('Score', ascending=False, inplace=True)
  for i in range(0, len(dataframe)):
    dataframe[i:i+1]['Total']+=i
  dataframe.sort_values('Purity', ascending=False, inplace=True)
  for j in range(0, len(dataframe)):
    dataframe[j:j+1]['Total']+=j
  dataframe.sort_values('Total', inplace=True)

# do process on I want 
class AutoProcess():
    def __init__(self, verbose=False):
        
        self.pp = PreprocessPipeline
        self.verbose = verbose

  #   def purity_score(y_true, y_pred):
  # # compute contingency matrix (also called confusion matrix)
  #     contingency_matrix = metrics.cluster.contingency_matrix(y_true,y_pred)
  #     return np.sum(np.amax(contingency_matrix, axis=0))/np.sum(contingency_matrix)

    def run(self, X,group):
        methods = []
        scores = []
        print(X.shape)

        def makePlt23(df, label, k, count, title):
            # list for store feature data for each cluster
            store = [[[] for col in range(len(df.columns))] for row in range(k)]

            for m in range(len(label)):
                for n in range(k):
                    if (label[m] == n):
                        for o in range(len(df.columns)):
                            store[n][o].append(df.iloc[m:m + 1, o:o + 1].values[0][0])

            c = ['b', 'r', 'g', 'y', 'c', 'm', 'k', 'limegreen', 'violet', 'dodgerblue', 'C1', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9']
            if (len(df.columns) == 2):
                plt.subplot(120 + count, title=title)
                plt.xlabel(df.columns[0])
                plt.ylabel(df.columns[1])
                for p in range(k):
                    plt.plot(store[p][0], store[p][1], '.', color=c[p])
            if (len(df.columns) == 3):
                plt.subplot(120 + count, projection='3d', title=title)
                plt.xlabel(df.columns[0])
                plt.ylabel(df.columns[1])
                for p in range(k):
                    plt.plot(store[p][0], store[p][1], store[p][2], '.', c[p])

        def makePltBig(df, label, k):
          if (len(df.columns) == 3):
              enc = preprocessing.OrdinalEncoder()
              op = enc.fit_transform(df['ocean_proximity'].to_numpy().reshape(-1, 1))
              df['ocean_proximity'] = op
          # list for store feature data for each cluster
          store = [[[] for col in range(len(df.columns))] for row in range(k)]

          for m in range(len(label)):
              for n in range(k):
                  if (label[m] == n):
                      for o in range(len(df.columns)):
                          store[n][o].append(df.iloc[m:m + 1, o:o + 1].values[0][0])

          if(len(df.columns)==2):
              for j in range(int(k/9)+1):
                  for i in range(j*9, (j+1)*9, 1):
                      if(i<k):
                          plt.subplot(330+(i-(j*9)+1), title='Cluster N.'+str(i+1))
                          plt.xlabel(df.columns[0])
                          plt.ylabel(df.columns[1])
                          plt.plot(store[i][0], store[i][1], '.')
                  plt.show()
          if (len(df.columns) == 3):
              for j in range(int(k / 9) + 1):
                  for i in range(j * 9, (j + 1) * 9, 1):
                      if (i < k):
                          plt.subplot(330 + (i - (j * 9) + 1), projection='3d', title='Cluster N.' + str(i + 1))
                          plt.xlabel(df.columns[0])
                          plt.ylabel(df.columns[1])
                          plt.plot(store[i][0], store[i][1], store[i][2], '.')
                  plt.show()
        
        for num_process in ['minmax']:# 'standard', 'maxabs', 'robust'
            for cat_process in ['ordinal']:#,'ondehot', 'Label'
                # if self.verbose:
                #     print("\n------------------------------------------------------\n")
                #     print(f"Numeric Process : {num_process}")
                #     print(f"Categorical Process : {cat_process}")
                methods.append([num_process, cat_process])

                pipeline = self.pp(num_process=num_process, cat_process=cat_process)
                
                X_processed= pipeline.process(X)
                
                #print(X_processed.shape)
                #Classifier part
                for model in ['k-mean', 'em','dbscan','mean-shift']:#,'em','dbscan','mean-shift''k-mean',,'mean-shift''dbscan','em','dbscan','mean-shift
                    # if self.verbose:
                    #     print(f"\nCluster model: {model}")
                    countPlt=0
                    if model =='k-mean': 
                        k_num = {11} #2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15
                        for k in k_num:
                            countPlt=countPlt+1
                            c_mdel = KMeans(n_clusters=k)
                            c_mdel.fit(X_processed)
                            sample = X.copy()
                            sample['cluster'] = c_mdel.labels_
                            sample_score = silhouette_samples(X_processed,sample['cluster'] )
                            sample['silhouette_'] = sample_score
                            sample_score = purity_score(y,sample['cluster'])
                            sample['purity'] = sample_score
                            score_results_list[0].loc[len(score_results)] = [group, num_process, cat_process, model,'k='+str(k), sample['silhouette_'].mean(), sample['purity'].mean(), 0]
                            makePlt23(X, sample['cluster'], k, countPlt, 'k='+str(k))
                        plt.show()


                    elif model == 'em':
                        k_num ={11} #2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15
                        countPlt = 0
                        for k in k_num:
                            countPlt+=1
                            c_mdel = GaussianMixture(n_components=k, random_state=0).fit(X_processed)
                            sample = X.copy()
                            c_mdel_cluster_labels = c_mdel.predict(X_processed)
                            sample['cluster'] = c_mdel_cluster_labels
                            sample_score = silhouette_samples(X_processed,sample['cluster'])
                            sample['silhouette_'] = sample_score
                            # score_results.loc[len(score_results)] = [group, num_process, cat_process, model,'k='+str(k), str(sample['silhouette_'].mean())]
                            sample_score = purity_score(y,sample['cluster'])
                            sample['purity'] = sample_score
                            score_results_list[1].loc[len(score_results_list[1])] = [group, num_process, cat_process, model,'k='+str(k), sample['silhouette_'].mean(), sample['purity'].mean(), 0]
                            makePlt23(X, sample['cluster'], k, countPlt, 'k='+str(k))
                        plt.show()
                  

                    elif model == 'dbscan':
                        esp = {0.9} #0.2 ,0.3, 0.4, 0.5, 0.6, 0.8, 0.9
                        ms = {2} #2, 3, 4, 5
                        for e in esp:
                          countPlt=0
                          for m in ms:
                            countPlt+=1
                            c_mdel = DBSCAN(eps = e, min_samples=m)
                            sample = X.copy()
                            # sample == preprocessing 전 dataframe
                            # X_processed == preprocessing 후 dataframe(columne 이름 다 0임)
                            sample['cluster'] = pd.DataFrame(c_mdel.fit_predict(X_processed))
                            # sample['cluster'] == cluster label
                            sample_score = silhouette_samples(X_processed,sample['cluster'])
                            sample['silhouette_'] = sample_score
                            sample_score = purity_score(y,sample['cluster'])
                            sample['purity'] = sample_score
                            # score_results.loc[len(score_results)] = [group, num_process, cat_process, model,'k='+str(k), str(sample['silhouette_'].mean()), str(sample['purity'].mean())]
                            score_results_list[2].loc[len(score_results_list[2])] = [group, num_process, cat_process, model,'eps:'+str(e)+' m:'+str(m)+' cluster:'+str(len(sample['cluster'].value_counts())), sample['silhouette_'].mean(), sample['purity'].mean(), 0]
                            k = len(sample[['cluster']].groupby('cluster').count())
                            makePlt23(X, sample['cluster'], k, countPlt, 'eps='+str(e)+' m='+str(m))
                        plt.show()
                              
                    elif model == 'mean-shift':
                        bandwidth={0.7} #0.1, 0.2, 0.3, 0.5, 0.7, 1
                        countPlt = 0
                        for b in bandwidth:
                          countPlt += 1
                          # best_bandwidth = estimate_bandwidth(X_processed)
                          c_mdel = MeanShift(bandwidth=b)
                          c_mdel_cluster_labels = c_mdel.fit_predict(X_processed)
                          sample = X.copy()
                          sample['cluster'] = c_mdel_cluster_labels

                          # print('cluster labels type: ', np.unique(c_mdel_cluster_labels))
                          # print('bandwidth값 : ',best_bandwidth)
                          sample_score = silhouette_samples(X_processed,sample['cluster'])
                          sample['silhouette_'] = sample_score
                          # print('aver sihouette_: ' +str(sample['silhouette_'].mean()))
                          # print(sample.groupby('cluster')['silhouette_'].mean())
                          # score_results.loc[len(score_results)] = [group, num_process, cat_process, model,'bandwidth: '+str(best_bandwidth), str(sample['silhouette_'].mean())]
                          sample_score = purity_score(y,sample['cluster'])
                          sample['purity'] = sample_score
                          # score_results.loc[len(score_results)] = [group, num_process, cat_process, model,'k='+str(k), str(sample['silhouette_'].mean()), str(sample['purity'].mean())]
                          score_results_list[3].loc[len(score_results_list[3])] = [group, num_process, cat_process, model,'bandwidth:'+str(b), sample['silhouette_'].mean(), sample['purity'].mean(), 0]
                          k = len(sample[['cluster']].groupby('cluster').count())
                          makePlt23(X, sample['cluster'], k, countPlt, 'bandwidth='+str(b))
                        plt.show()
        print('------------------------------------------------------')
        for i in range(0,4):
          makeOrderList(score_results_list[i])
                    


        return

################
#make result table
score_sample = {'Group':["Group"],'Scaler':["Sample"], 'Encoder':["Sample"], 'Model':["Sample"],'Best_para':["Sample"], "Score":[1], "Purity":[1], "Total":[0]}
score_results = pd.DataFrame(score_sample)
srCopy=score_results.copy()
srCopy2=score_results.copy()
srCopy3=score_results.copy()
score_results_list=[score_results, srCopy, srCopy2, srCopy3]
score_sample2 = {'type':["error"],'info':["info"]}
error_data = pd.DataFrame(score_sample2)

print(score_results_list[0])
# kfold = KFold(5, True, 1)

# pd.set_option('display.max_row', 10000)
# Import the data file
df = pd.read_csv('bank.csv', na_values='unknown')
# print(df.dtypes)
# print(df.isna().sum())

# show all columns 
pd.options.display.max_columns = None
# 잘려 보이는거 해결
pd.options.display.max_colwidth=-1
pd.options.display.width=None
# pd.set_option('display.width', None)

##setting data set
#ex = df.iloc[:,8]
# drop default and poutcome columns
df.drop('default', axis=1, inplace=True)
df.drop('poutcome', axis=1, inplace=True)

df.dropna(inplace=True)
# print(df.info())

# set index again
df.index=np.arange(0, len(df))

x=df.drop(['deposit'], axis=1)
y=df['deposit']

# print(df.isna().sum())
#group 1 room
# X1 = df[['total_rooms','total_bedrooms']]
X1 = df[['balance','job']]


# # #group 2 where
# X2 = df[['housing','loan']]

# # #group 3 spec
X3 = df[['month','duration']]


# #group 5 all
# X5 = df
# print(score_results)
autoprocess = AutoProcess(verbose=True)
autoprocess.run(X1,'income')
# autoprocess.run(X2,'where')
# autoprocess.run(X3,'spec')
# autoprocess.run(X4,'eviroment')
# autoprocess.run(X5,'all')
for i in range(0, 4):
  print(score_results_list[i])
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')

# sys.stdout = open('E:\PythonWorkSpace\score result.txt', 'w')


# sys.stdout.close()

# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16TBY3u5wAAnNzaNcfwASt2YZ1Me-bOKj
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("bank.csv")

def encoder(df):
    from sklearn.preprocessing import LabelEncoder

    cate = list(df.select_dtypes(include=['object']).columns)
    label_feat = {}
    for i, feature in enumerate(cate):
        label_feat[feature] = LabelEncoder()
        df[feature] = label_feat[feature].fit_transform(df[feature])

    return df

test_encoded_df = encoder(df)

df.head()

#check for null and unknown value in columns
print("<Null value in dataframe>")
print(df.isnull().sum())
print("\n<Unknown values in dataframe>")
for c in df.columns:
  print("Unknown value in ", c, ": ", len(df[df[c] == 'unknown'].index.tolist()))

distribution = test_encoded_df.hist(linewidth=1, color='c')
fig = plt.gcf()
fig.set_size_inches(12, 12)
plt.show()

from sklearn.model_selection import train_test_split

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("bank.csv")

def encoder(df):
    from sklearn.preprocessing import LabelEncoder

    cate = list(df.select_dtypes(include=['object']).columns)
    label_feat = {}
    for i, feature in enumerate(cate):
        label_feat[feature] = LabelEncoder()
        df[feature] = label_feat[feature].fit_transform(df[feature])

    return df


df = df[df['job'] != 'unknown']
df = df[df['contact'] != 'unknown']
df = df[df['education'] != 'unknown']

df.drop('default', axis=1, inplace=True)
df.drop('poutcome', axis=1, inplace=True)

df = encoder(df)

x = df.drop(['deposit'], axis=1)
y = df['deposit']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True, random_state=10)

from xgboost import XGBClassifier
from xgboost import plot_importance

model = XGBClassifier()
model_importance = model.fit(x_train, y_train)

plt.rcParams['figure.figsize'] = [15, 12]
sns.set(style = 'darkgrid')
print(plot_importance(model_importance))

plt.rcParams['figure.figsize'] = [15, 12]
sns.heatmap(test_encoded_df.corr(), annot = True, color='Black')
plt.title('Feature Correlation')
plt.show()

import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score

from sklearn.tree import export_graphviz
import graphviz
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score, plot_confusion_matrix


fpr = [0, 0, 0]
tpr = [0, 0, 0]
roc_auc = [0, 0, 0]
index = 0

classification_models = ['dtree', 'nb', 'svm']
decisionParam = {'max_depth':[3, 4, 5, 6, 7, 8], 'min_sample_leaf':[1, 2, 3, 4, 5],
                'min_samples_split':[2, 3, 4, 5, 6, 7]}
gaussianParam = {'var_smoothing':[1e-12, 1e-11, 1e-10, 1e-9, 1e-8]}
SVCParam =  {'C':[0.01, 0.1, 1, 10], 'gamma':[0.01, 0.1, 1, 10]}

Model = ['DecisionTree', 'GaussianNB', 'Support Vector machine']
bestScore = [0, 0, 0]
bestParams = ["d", "g", "s"]
c_matrix = [0, 0, 0]


def classification(x, y, model):
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import f1_score,classification_report,confusion_matrix

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True, random_state=10)
    
    if model == 'dtree':
      currentBest = 0.0
      for md in decisionParam['max_depth']:
        for msl in decisionParam['min_sample_leaf']:
          for mss in decisionParam['min_samples_split']:
            tree = DecisionTreeClassifier(max_depth=md, min_samples_leaf=msl, min_samples_split=mss, random_state=12)
            tree.fit(x_train, y_train)
            tree_preds = tree.predict(x_test)
            tree_prob = tree.predict_proba(x_test)
            prob = tree_prob[:, 1]

            score = f1_score(y_test, tree_preds, pos_label='yes')
            if(score > currentBest):
              currentBest = score
              bestScore[0] = score
              bestParams[0] = 'max_depth:'+str(md)+' min_sample_leaf:'+str(msl)+' min_samples_split:'+str(mss)
              c_matrix[0] = confusion_matrix(y_test, tree_preds)
              #parameters in roc curve
              global index
              le =  LabelEncoder()
              yt = le.fit_transform(y_test)
              fpr[index], tpr[index], _ = roc_curve(yt, prob)
              #confusion matrix visualization
              plot_confusion_matrix(tree, x_test, y_test)
              plt.title("D tree")
              plt.savefig('tree_model.png')
              plt.show()

    elif model == 'nb':
      currentBest = 0.0
      for vs in gaussianParam['var_smoothing']:
        nb = GaussianNB(var_smoothing=vs)
        nb.fit(x_train, y_train)
        nb_preds = nb.predict(x_test)
        nb_prob = nb.predict_proba(x_test)
        prob = nb_prob[:, 1]

        score = f1_score(y_test, nb_preds, pos_label='yes')
        if(score > currentBest):
          currentBest = score
          bestScore[1] = score
          bestParams[1] = 'var_smoothing:'+str(vs)
          c_matrix[1] = confusion_matrix(y_test, nb_preds)
          #parameters in roc curve
          index = 1
          le =  LabelEncoder()
          yt = le.fit_transform(y_test)
          fpr[index], tpr[index], _ = roc_curve(yt, prob)
          #confusion matrix visualization
          plot_confusion_matrix(nb, x_test, y_test)
          plt.title("Naive Bayes")
          plt.savefig('nb_model.png')
          plt.show()

    elif model == 'svm':
      currentBest = 0.0
      for c in SVCParam['C']:
        for g in SVCParam['gamma']:
          svm = SVC(C=c, gamma=g, probability=True)
          svm.fit(x_train, y_train)
          svm_preds = svm.predict(x_test)
          svm_prob = svm.predict_proba(x_test)
          prob = svm_prob[:, 1]

          score = f1_score(y_test, svm_preds, pos_label='yes')
          if(score > currentBest):
            currentBest = score
            bestScore[2] = score
            bestParams[2] = 'C:'+str(c)+' gamma:'+str(g)
            c_matrix[2] = confusion_matrix(y_test, svm_preds)
            #parameters in roc curve
            index = 2
            le =  LabelEncoder()
            yt = le.fit_transform(y_test)
            fpr[index], tpr[index], _ = roc_curve(yt, prob)
            #confusion matrix visualization
            plot_confusion_matrix(svm, x_test, y_test)
            plt.title("Support vector machine")
            plt.savefig('svm_model.png')
            plt.show()
    else:
        print('Invalid classification model name...')
    

# function label encoding
# input target column list and dataframe
def lblEncoding(listObj, x):
    lbl = preprocessing.LabelEncoder()

    for i in range(len(listObj)):
        x[listObj[i]] = lbl.fit_transform(x[listObj[i]])
    # output encoded dataframe
    return x

# function ordinal encoding
# input target column list and dataframe
def ordEncoding(listObj, x):
    ord=preprocessing.OrdinalEncoder()

    for i in range(len(listObj)):
        tempColumn=x[listObj[i]].to_numpy().reshape(-1, 1)
        tempColumn=ord.fit_transform(tempColumn)
        tempColumn=tempColumn.reshape(1, -1)[0]
        x[listObj[i]].replace(x[listObj[i]].tolist(), tempColumn, inplace=True)
    # output encoded dataframe
    return x

# function ohehot encoding
# input dataframe
def ohEncoding(x):
    # output encoded dataframe
    return pd.get_dummies(x)


def find_best(df):
    x = df.drop(['deposit'], axis=1)
    y = df['deposit']
    listObj=['job', 'marital', 'education', 'housing', 'loan', 'contact', 'month']
    scalerTemp = [preprocessing.RobustScaler(), preprocessing.MaxAbsScaler(), preprocessing.StandardScaler()]

    for i in range(0, len(classification_models)):
      if(listEncoder[i] == 'LabelEncoder'):
        encoded_df = lblEncoding(listObj, x)
      else:
        encoded_df = ohEncoding(x)

      scaler = scalerTemp[i]
      X = scaler.fit_transform(encoded_df)
      classification(X, y, classification_models[i])

    #print results
    for i in range(0, len(Model)):
      print("\n<Best score and params in ", Model[i], ">")
      print("Confusion Matrix:\n", c_matrix[i])
      print("F1 score:", bestScore[i])
      print("Best params:", bestParams[i])
    





df = pd.read_csv("bank.csv")

df = df[df['job'] != 'unknown']
df = df[df['contact'] != 'unknown']
df = df[df['education'] != 'unknown']

df.drop('default', axis=1, inplace=True)
df.drop('poutcome', axis=1, inplace=True)

# user can select classifier, encoder and scaler to make combination
listClassifier=[DecisionTreeClassifier(), GaussianNB(), SVC()]
listEncoder=['LabelEncoder', 'OneHotEncoder', 'OneHotEncoder']
listScaler=['RobustScaler', 'MaxAbsScaler', 'StandardScaler']


find_best(df)

import graphviz
with open("tree.dot") as f:
      dot_graph = f.read()
graphviz.Source(dot_graph).render('tree', format='png')

fig, ax = plt.subplots(figsize = (20, 10))
plt.plot([0, 1], [0, 1], linestyle='--')
plt.plot(fpr[0], tpr[0], marker='.', label = 'DecisionTree')
plt.plot(fpr[1], tpr[1], marker='.', label = 'GaussianNB')
plt.plot(fpr[2], tpr[2], marker='.', label = 'SVC')
plt.legend(fontsize = 18)
plt.title("Models Roc Curve", fontsize = 25)
plt.show()

!pip install visuaize

